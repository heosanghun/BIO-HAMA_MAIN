"""
ì„ í˜¸ë„ ëª¨ë¸ê³¼ ë³´ìƒ ê³„ì‚°ê¸° í…ŒìŠ¤íŠ¸

ê¸°ë³¸ ì„ í˜¸ë„ ëª¨ë¸ê³¼ ë³´ìƒ ê³„ì‚°ê¸°ì˜ ê¸°ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import torch
import numpy as np
import sys
import os

# BioHama ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from biohama.training.preference_model import PreferenceModel
from biohama.training.reward_calculator import RewardCalculator


def test_preference_model():
    """ì„ í˜¸ë„ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì„ í˜¸ë„ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ì„¤ì •
    config = {
        'input_dim': 512,
        'hidden_dim': 128,
        'embedding_dim': 64
    }
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    preference_model = PreferenceModel(config, device)
    preference_model.to(device)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size = 4
    input_data = torch.randn(batch_size, config['input_dim'], device=device)
    
    # ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸
    prediction, confidence = preference_model(input_data)
    
    print(f"âœ… ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì˜ˆì¸¡ê°’ í˜•íƒœ: {prediction.shape}")
    print(f"   - ì‹ ë¢°ë„ í˜•íƒœ: {confidence.shape}")
    print(f"   - ì˜ˆì¸¡ê°’ ë²”ìœ„: {prediction.min().item():.4f} ~ {prediction.max().item():.4f}")
    print(f"   - ì‹ ë¢°ë„ ë²”ìœ„: {confidence.min().item():.4f} ~ {confidence.max().item():.4f}")
    
    # ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
    outputs = {
        'attention_output': torch.randn(batch_size, 64, 512, device=device),
        'decision_output': torch.randn(batch_size, 10, device=device)
    }
    
    preferences = {
        'attention_output': 0.8,
        'decision_output': 0.9
    }
    
    loss = preference_model.update(outputs, preferences)
    
    print(f"âœ… ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì†ì‹¤ê°’: {loss:.4f}")
    
    # ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸
    summary = preference_model.get_preference_summary()
    print(f"âœ… ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì—…ë°ì´íŠ¸ íšŸìˆ˜: {summary['update_count']}")
    print(f"   - í›ˆë ¨ ëª¨ë“œ: {summary['training_mode']}")
    
    return True


def test_reward_calculator():
    """ë³´ìƒ ê³„ì‚°ê¸° í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ë³´ìƒ ê³„ì‚°ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ì„¤ì •
    config = {
        'accuracy_reward': {
            'accuracy_threshold': 0.8,
            'reward_scale': 1.0
        },
        'efficiency_reward': {
            'efficiency_threshold': 0.7,
            'reward_scale': 0.5
        },
        'consistency_reward': {
            'consistency_threshold': 0.6,
            'reward_scale': 0.3
        },
        'reward_weights': {
            'accuracy': 0.5,
            'efficiency': 0.3,
            'consistency': 0.2
        }
    }
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ê³„ì‚°ê¸° ì´ˆê¸°í™”
    reward_calculator = RewardCalculator(config, device)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size = 4
    outputs = {
        'attention_output': torch.randn(batch_size, 64, 512, device=device),
        'attention_mask': torch.rand(batch_size, 8, 64, 64, device=device) > 0.5,
        'computation_savings': 0.75,
        'should_terminate': torch.rand(batch_size, device=device) > 0.5,
        'confidence': torch.rand(batch_size, device=device),
        'quality': torch.rand(batch_size, device=device)
    }
    
    targets = torch.randint(0, 10, (batch_size,), device=device)
    
    # ë³´ìƒ ê³„ì‚° í…ŒìŠ¤íŠ¸
    rewards = reward_calculator.calculate_rewards(outputs, targets)
    
    print(f"âœ… ë³´ìƒ ê³„ì‚° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ë³´ìƒ í…ì„œ í˜•íƒœ: {rewards.shape}")
    print(f"   - ë³´ìƒ ë²”ìœ„: {rewards.min().item():.4f} ~ {rewards.max().item():.4f}")
    print(f"   - í‰ê·  ë³´ìƒ: {rewards.mean().item():.4f}")
    
    # ì„ í˜¸ë„ ê¸°ë°˜ ë³´ìƒ í…ŒìŠ¤íŠ¸
    preferences = {
        'accuracy': 0.8,
        'efficiency': 0.6,
        'consistency': 0.7
    }
    
    rewards_with_prefs = reward_calculator.calculate_rewards(outputs, targets, preferences)
    
    print(f"âœ… ì„ í˜¸ë„ ê¸°ë°˜ ë³´ìƒ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì„ í˜¸ë„ ì ìš© ì „ í‰ê· : {rewards.mean().item():.4f}")
    print(f"   - ì„ í˜¸ë„ ì ìš© í›„ í‰ê· : {rewards_with_prefs.mean().item():.4f}")
    
    # ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸
    summary = reward_calculator.get_reward_summary()
    print(f"âœ… ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì´ ê³„ì‚° íšŸìˆ˜: {summary['reward_stats']['total_calculations']}")
    print(f"   - í‰ê·  ì •í™•ë„ ë³´ìƒ: {summary['reward_stats']['avg_accuracy_reward']:.4f}")
    print(f"   - í‰ê·  íš¨ìœ¨ì„± ë³´ìƒ: {summary['reward_stats']['avg_efficiency_reward']:.4f}")
    print(f"   - í‰ê·  ì¼ê´€ì„± ë³´ìƒ: {summary['reward_stats']['avg_consistency_reward']:.4f}")
    
    return True


def test_integration():
    """í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    preference_config = {
        'input_dim': 512,
        'hidden_dim': 128,
        'embedding_dim': 64
    }
    
    reward_config = {
        'accuracy_reward': {'accuracy_threshold': 0.8, 'reward_scale': 1.0},
        'efficiency_reward': {'efficiency_threshold': 0.7, 'reward_scale': 0.5},
        'consistency_reward': {'consistency_threshold': 0.6, 'reward_scale': 0.3},
        'reward_weights': {'accuracy': 0.5, 'efficiency': 0.3, 'consistency': 0.2}
    }
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    preference_model = PreferenceModel(preference_config, device)
    reward_calculator = RewardCalculator(reward_config, device)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    batch_size = 4
    outputs = {
        'attention_output': torch.randn(batch_size, 64, 512, device=device),
        'attention_mask': torch.rand(batch_size, 8, 64, 64, device=device) > 0.5,
        'computation_savings': 0.75,
        'should_terminate': torch.rand(batch_size, device=device) > 0.5,
        'confidence': torch.rand(batch_size, device=device),
        'quality': torch.rand(batch_size, device=device)
    }
    
    targets = torch.randint(0, 10, (batch_size,), device=device)
    
    # 1. ë³´ìƒ ê³„ì‚°
    rewards = reward_calculator.calculate_rewards(outputs, targets)
    
    # 2. ì„ í˜¸ë„ ëª¨ë¸ ì—…ë°ì´íŠ¸
    preferences = {
        'attention_output': 0.8,
        'decision_output': 0.9
    }
    
    preference_loss = preference_model.update(outputs, preferences, rewards)
    
    print(f"âœ… í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ê³„ì‚°ëœ ë³´ìƒ: {rewards.mean().item():.4f}")
    print(f"   - ì„ í˜¸ë„ ì†ì‹¤: {preference_loss:.4f}")
    
    # 3. ëª¨ë¸ ìš”ì•½ ì •ë³´
    pref_summary = preference_model.get_preference_summary()
    reward_summary = reward_calculator.get_reward_summary()
    
    print(f"   - ì„ í˜¸ë„ ëª¨ë¸ ì—…ë°ì´íŠ¸: {pref_summary['update_count']}íšŒ")
    print(f"   - ë³´ìƒ ê³„ì‚° íšŸìˆ˜: {reward_summary['reward_stats']['total_calculations']}íšŒ")
    
    return True


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ ì„ í˜¸ë„ ëª¨ë¸ê³¼ ë³´ìƒ ê³„ì‚°ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    try:
        # ê°œë³„ í…ŒìŠ¤íŠ¸
        test_preference_model()
        test_reward_calculator()
        test_integration()
        
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("âœ… ì„ í˜¸ë„ ëª¨ë¸: ê¸°ë³¸ ê¸°ëŠ¥ ì •ìƒ ì‘ë™")
        print("âœ… ë³´ìƒ ê³„ì‚°ê¸°: ê¸°ë³¸ ê¸°ëŠ¥ ì •ìƒ ì‘ë™")
        print("âœ… í†µí•© í…ŒìŠ¤íŠ¸: ëª¨ë¸ ê°„ ì—°ë™ ì •ìƒ ì‘ë™")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
