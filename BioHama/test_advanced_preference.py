"""
ê³ ê¸‰ ì„ í˜¸ë„ ëª¨ë¸ í…ŒìŠ¤íŠ¸

ì„ í˜¸ë„ í•™ìŠµê¸°ì™€ ë©”ëª¨ë¦¬ë¥¼ í¬í•¨í•œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import torch
import numpy as np
import sys
import os

# BioHama ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from biohama.training.preference_model import PreferenceModel
from biohama.training.preference_learner import PreferenceLearner
from biohama.training.preference_memory import PreferenceMemory, PreferenceData


def test_preference_learner():
    """ì„ í˜¸ë„ í•™ìŠµê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì„ í˜¸ë„ í•™ìŠµê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # í•™ìŠµê¸° ì´ˆê¸°í™”
    learner = PreferenceLearner(
        embedding_dim=64,
        learning_rate=1e-4,
        memory_size=1000
    )
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    current_pref = torch.randn(4, 64, device=device)
    target_pref = torch.randn(4, 64, device=device)
    
    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´
    context = {
        'confidence': 0.8,
        'reward': 0.6,
        'time_since_last_update': 2.0
    }
    
    # ì˜¨ë¼ì¸ í•™ìŠµ í…ŒìŠ¤íŠ¸
    updated_pref = learner.update_preference(
        current_pref, target_pref, 'online', context
    )
    
    print(f"âœ… ì˜¨ë¼ì¸ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì—…ë°ì´íŠ¸ ì „ ì°¨ì´: {torch.norm(current_pref - target_pref).item():.4f}")
    print(f"   - ì—…ë°ì´íŠ¸ í›„ ì°¨ì´: {torch.norm(updated_pref - target_pref).item():.4f}")
    
    # ë°°ì¹˜ í•™ìŠµ í…ŒìŠ¤íŠ¸
    updated_pref_batch = learner.update_preference(
        current_pref, target_pref, 'batch', context
    )
    
    print(f"âœ… ë°°ì¹˜ í•™ìŠµ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ë°°ì¹˜ ì—…ë°ì´íŠ¸ í›„ ì°¨ì´: {torch.norm(updated_pref_batch - target_pref).item():.4f}")
    
    # í•™ìŠµ í†µê³„ í…ŒìŠ¤íŠ¸
    stats = learner.get_learning_stats()
    print(f"âœ… í•™ìŠµ í†µê³„ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì´ ì—…ë°ì´íŠ¸: {stats['total_updates']}")
    print(f"   - ìˆ˜ë ´ë¥ : {stats['convergence_rate']:.4f}")
    print(f"   - í‰ê·  ì†ì‹¤: {stats['avg_loss']:.4f}")
    
    return True


def test_preference_memory():
    """ì„ í˜¸ë„ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ì„ í˜¸ë„ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    memory = PreferenceMemory(
        max_size=1000,
        memory_type='fifo',
        similarity_threshold=0.7
    )
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_data = []
    for i in range(10):
        data = PreferenceData(
            preference_type='explicit' if i % 2 == 0 else 'implicit',
            input_data=torch.randn(4, 64, device=device),
            preference_value=0.5 + i * 0.1,
            timestamp=time.time() + i,
            context={'test_id': i}
        )
        test_data.append(data)
    
    # ë°ì´í„° ì €ì¥ í…ŒìŠ¤íŠ¸
    stored_keys = []
    for i, data in enumerate(test_data):
        key = memory.store(data, f"test_key_{i}")
        stored_keys.append(key)
        
    print(f"âœ… ë°ì´í„° ì €ì¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì €ì¥ëœ ë°ì´í„° ìˆ˜: {len(stored_keys)}")
    
    # ë°ì´í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    retrieved_data = memory.retrieve(stored_keys[0])
    print(f"âœ… ë°ì´í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ê²€ìƒ‰ëœ ë°ì´í„° íƒ€ì…: {retrieved_data.preference_type}")
    print(f"   - ê²€ìƒ‰ëœ ë°ì´í„° ê°’: {retrieved_data.preference_value}")
    
    # ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    query_data = torch.randn(4, 64, device=device)
    similar_data = memory.retrieve_similar(query_data, top_k=3)
    
    print(f"âœ… ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ìœ ì‚¬í•œ ë°ì´í„° ìˆ˜: {len(similar_data)}")
    if similar_data:
        print(f"   - ìµœê³  ìœ ì‚¬ë„: {similar_data[0][2]:.4f}")
    
    # íƒ€ì…ë³„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    explicit_data = memory.retrieve_by_type('explicit', limit=3)
    implicit_data = memory.retrieve_by_type('implicit', limit=3)
    
    print(f"âœ… íƒ€ì…ë³„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ëª…ì‹œì  ë°ì´í„°: {len(explicit_data)}ê°œ")
    print(f"   - ì•”ì‹œì  ë°ì´í„°: {len(implicit_data)}ê°œ")
    
    # ë©”ëª¨ë¦¬ í†µê³„ í…ŒìŠ¤íŠ¸
    stats = memory.get_memory_stats()
    print(f"âœ… ë©”ëª¨ë¦¬ í†µê³„ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - íˆíŠ¸ìœ¨: {stats['hit_rate']:.4f}")
    print(f"   - í˜„ì¬ í¬ê¸°: {stats['current_size']}")
    print(f"   - íƒ€ì… ë¶„í¬: {stats['type_distribution']}")
    
    return True


def test_advanced_preference_model():
    """ê³ ê¸‰ ì„ í˜¸ë„ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ê³ ê¸‰ ì„ í˜¸ë„ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ê³ ê¸‰ ì„¤ì •
    config = {
        'input_dim': 512,
        'hidden_dim': 128,
        'embedding_dim': 64,
        'learning_rate': 1e-4,
        'learner_memory_size': 1000,
        'memory_size': 5000,
        'memory_type': 'fifo',
        'similarity_threshold': 0.8
    }
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    preference_model = PreferenceModel(config, device)
    preference_model.to(device)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    batch_size = 4
    outputs = {
        'attention_output': torch.randn(batch_size, 64, 512, device=device),
        'attention_mask': torch.rand(batch_size, 8, 64, 64, device=device) > 0.5,
        'computation_savings': 0.75,
        'should_terminate': torch.rand(batch_size, device=device) > 0.5,
        'confidence': torch.rand(batch_size, device=device),
        'quality': torch.rand(batch_size, device=device)
    }
    
    preferences = {
        'attention_output': 0.8,
        'decision_output': 0.9
    }
    
    rewards = torch.rand(batch_size, device=device)
    
    # ê³ ê¸‰ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
    loss = preference_model.update(outputs, preferences, rewards)
    
    print(f"âœ… ê³ ê¸‰ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì†ì‹¤ê°’: {loss:.4f}")
    
    # ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸
    summary = preference_model.get_preference_summary()
    print(f"âœ… ìš”ì•½ ì •ë³´ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì—…ë°ì´íŠ¸ íšŸìˆ˜: {summary['update_count']}")
    print(f"   - í•™ìŠµ í†µê³„: {summary['learner_stats']['total_updates']}íšŒ ì—…ë°ì´íŠ¸")
    print(f"   - ë©”ëª¨ë¦¬ í†µê³„: {summary['memory_stats']['current_size']}ê°œ ì €ì¥")
    
    # ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸
    preference_model.memory.optimize_memory()
    print(f"âœ… ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    
    return True


def test_integration():
    """í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ê³ ê¸‰ ì„¤ì •
    config = {
        'input_dim': 512,
        'hidden_dim': 128,
        'embedding_dim': 64,
        'learning_rate': 1e-4,
        'learner_memory_size': 1000,
        'memory_size': 5000,
        'memory_type': 'lru',  # LRU ë©”ëª¨ë¦¬ ì‚¬ìš©
        'similarity_threshold': 0.8
    }
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    preference_model = PreferenceModel(config, device)
    
    # ì—°ì† ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
    for epoch in range(5):
        batch_size = 4
        outputs = {
            'attention_output': torch.randn(batch_size, 64, 512, device=device),
            'attention_mask': torch.rand(batch_size, 8, 64, 64, device=device) > 0.5,
            'computation_savings': 0.75,
            'should_terminate': torch.rand(batch_size, device=device) > 0.5,
            'confidence': torch.rand(batch_size, device=device),
            'quality': torch.rand(batch_size, device=device)
        }
        
        preferences = {
            'attention_output': 0.8 + epoch * 0.02,
            'decision_output': 0.9 - epoch * 0.01
        }
        
        rewards = torch.rand(batch_size, device=device)
        
        loss = preference_model.update(outputs, preferences, rewards)
        
        if epoch % 2 == 0:
            print(f"   - Epoch {epoch}: ì†ì‹¤ = {loss:.4f}")
    
    # ìµœì¢… í†µê³„
    summary = preference_model.get_preference_summary()
    
    print(f"âœ… í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    print(f"   - ì´ ì—…ë°ì´íŠ¸: {summary['update_count']}íšŒ")
    print(f"   - í•™ìŠµ ìˆ˜ë ´ë¥ : {summary['learner_stats']['convergence_rate']:.4f}")
    print(f"   - ë©”ëª¨ë¦¬ íˆíŠ¸ìœ¨: {summary['memory_stats']['hit_rate']:.4f}")
    print(f"   - ë©”ëª¨ë¦¬ í¬ê¸°: {summary['memory_stats']['current_size']}")
    
    return True


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ ê³ ê¸‰ ì„ í˜¸ë„ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    try:
        # ê°œë³„ í…ŒìŠ¤íŠ¸
        test_preference_learner()
        test_preference_memory()
        test_advanced_preference_model()
        test_integration()
        
        print("\nğŸ‰ ëª¨ë“  ê³ ê¸‰ í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("âœ… ì„ í˜¸ë„ í•™ìŠµê¸°: ì˜¨ë¼ì¸/ë°°ì¹˜ í•™ìŠµ ì •ìƒ ì‘ë™")
        print("âœ… ì„ í˜¸ë„ ë©”ëª¨ë¦¬: FIFO/LRU ë©”ëª¨ë¦¬ ì •ìƒ ì‘ë™")
        print("âœ… ê³ ê¸‰ ì„ í˜¸ë„ ëª¨ë¸: í†µí•© ê¸°ëŠ¥ ì •ìƒ ì‘ë™")
        print("âœ… í†µí•© í…ŒìŠ¤íŠ¸: ì „ì²´ ì‹œìŠ¤í…œ ì—°ë™ ì •ìƒ ì‘ë™")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    import time
    success = main()
    sys.exit(0 if success else 1)
